{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXXTuy_o0sjk"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U kaggle_environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "yz23vWHD0wcj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from kaggle_environments import make, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kuo6IOxiRub"
   },
   "source": [
    "Стратегия 1: Всегда выбирает \"камень\" (значение 0) на каждом ходу.\n",
    "\n",
    "Описание: Очень простая и предсказуемая стратегия. Агент не реагирует на действия противника и не использует никакой информации о предыдущих ходах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "bqTqV7B92rJ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rock_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rock_agent.py\n",
    "\n",
    "def your_agent(observation, configuration):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 2: Всегда выбирает \"бумагу\" (значение 1) на каждом ходу.\n",
    "\n",
    "Описание: Очень простая и предсказуемая стратегия. Агент не реагирует на действия противника и не использует никакой информации о предыдущих ходах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting paper_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile paper_agent.py\n",
    "\n",
    "def paper_agent(observation, configuration):\n",
    "    return 1  # 1 соответствует \"бумага\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 3: Всегда выбирает \"ножницы\" (значение 2) на каждом ходу.\n",
    "\n",
    "Описание: Очень простая и предсказуемая стратегия. Агент не реагирует на действия противника и не использует никакой информации о предыдущих ходах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scissors_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scissors_agent.py\n",
    "\n",
    "def scissors_agent(observation, configuration):\n",
    "    return 2  # 2 соответствует \"ножницы\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 4: На каждом ходу случайно выбирает одно из возможных действий: \"камень\", \"бумагу\" или \"ножницы\".\n",
    "\n",
    "Описание: Использует генератор случайных чисел для выбора действия. Поведение агента непредсказуемо для противника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting random_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile random_agent.py\n",
    "\n",
    "import random\n",
    "\n",
    "def random_agent(observation, configuration):\n",
    "    return random.randrange(0, configuration.signs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 5: Последовательно перебирает действия в порядке: \"камень\" $\\rightarrow$ \"бумага\" $\\rightarrow$ \"ножницы\" $\\rightarrow$ повтор.\n",
    "\n",
    "Описание: Использует текущий номер хода (observation.step) для определения следующего действия по модулю количества знаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cycle_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cycle_agent.py\n",
    "\n",
    "def cycle_agent(observation, configuration):\n",
    "    return observation.step % configuration.signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 6: Выбирает действие, которое побеждает последнее действие противника.\n",
    "\n",
    "Описание: Если противник на прошлом ходу выбрал \"камень\", агент выберет \"бумагу\", чтобы победить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting beat_last_move_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile beat_last_move_agent.py\n",
    "\n",
    "def beat_last_move_agent(observation, configuration):\n",
    "    if observation.step > 0:\n",
    "        return (observation.lastOpponentAction + 1) % configuration.signs\n",
    "    else:\n",
    "        return 0  # Начинает с \"камня\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 7: Выбирает действие, которое проигрывает последнему действию противника.\n",
    "\n",
    "Описание: Если противник выбрал \"бумагу\", агент выберет \"камень\", который проиграет \"бумаге\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lose_to_last_move_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lose_to_last_move_agent.py\n",
    "\n",
    "def lose_to_last_move_agent(observation, configuration):\n",
    "    if observation.step > 0:\n",
    "        return (observation.lastOpponentAction + 2) % configuration.signs\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 8: Отслеживает, какое действие противник использует чаще всего, и выбирает действие, которое побеждает его.\n",
    "\n",
    "Описание: Считает количество каждого действия противника и выбирает контрдействие против самого частого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequency_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequency_agent.py\n",
    "\n",
    "def frequency_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "    if observation.step == 0:\n",
    "        frequency_agent.counts = [0] * configuration.signs\n",
    "    else:\n",
    "        frequency_agent.counts[observation.lastOpponentAction] += 1\n",
    "    most_common = frequency_agent.counts.index(max(frequency_agent.counts))\n",
    "    return (most_common + 1) % configuration.signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et1J5hUGigeh"
   },
   "source": [
    "Стратегия 9: Повторяет последнее действие противника.\n",
    "\n",
    "Описание: Если противник на прошлом ходу выбрал \"ножницы\", агент тоже выберет \"ножницы\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "7l6Ttw6qi0jk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting copy_opponent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile copy_opponent.py\n",
    "\n",
    "import random\n",
    "\n",
    "#Example\n",
    "def copy_opponent(observation, configuration):\n",
    "    #in case we have information about opponent last move\n",
    "    if observation.step > 0:\n",
    "        return observation.lastOpponentAction\n",
    "    #initial step\n",
    "    else:\n",
    "        return random.randrange(0, configuration.signs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 10: Отслеживает, какое действие противник использует реже всего, и выбирает действие, которое побеждает его.\n",
    "\n",
    "Описание: Нацелен на использование слабостей в игре противника, предполагая, что редко используемые действия могут быть неожиданными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting anti_frequency_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile anti_frequency_agent.py\n",
    "\n",
    "def anti_frequency_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "    if observation.step == 0:\n",
    "        anti_frequency_agent.counts = [0] * configuration.signs\n",
    "    else:\n",
    "        anti_frequency_agent.counts[observation.lastOpponentAction] += 1\n",
    "    least_common = anti_frequency_agent.counts.index(min(anti_frequency_agent.counts))\n",
    "    return (least_common + 1) % configuration.signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 11: Выбирает действие, которое побеждает его собственное предыдущее действие.\n",
    "\n",
    "Описание: Если агент на прошлом ходу выбрал \"камень\", то на следующем выберет \"бумагу\", чтобы победить \"камень\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mirror_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mirror_agent.py\n",
    "\n",
    "import random\n",
    "\n",
    "def mirror_agent(observation, configuration):\n",
    "    global my_last_action\n",
    "    if observation.step == 0:\n",
    "        # На первом шаге выбираем случайное действие и сохраняем его\n",
    "        action = random.randrange(0, configuration.signs)\n",
    "        my_last_action = action\n",
    "    else:\n",
    "        # Выбираем действие, которое побеждает наше предыдущее действие\n",
    "        action = (my_last_action + 1) % configuration.signs\n",
    "        # Обновляем наше последнее действие для следующего хода\n",
    "        my_last_action = action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 12: Начинает с \"камня\", затем всегда повторяет предыдущее действие противника.\n",
    "\n",
    "Описание: Стратегия основана на взаимности и копировании поведения противника после первого хода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tit_for_tat_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tit_for_tat_agent.py\n",
    "\n",
    "def tit_for_tat_agent(observation, configuration):\n",
    "    if observation.step > 0:\n",
    "        return observation.lastOpponentAction\n",
    "    else:\n",
    "        return 0  # Начинает с \"камня\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 13: Чаще выбирает \"камень\" по сравнению с другими действиями.\n",
    "\n",
    "Описание: Вероятность выбора действий распределена неравномерно: \"камень\" — 50%, \"бумага\" — 25%, \"ножницы\" — 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting biased_random_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile biased_random_agent.py\n",
    "\n",
    "import random\n",
    "\n",
    "def biased_random_agent(observation, configuration):\n",
    "    choices = [0] * 50 + [1] * 25 + [2] * 25  # 50% камень, 25% бумага, 25% ножницы\n",
    "    return random.choice(choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 14: Следует заранее определённой последовательности действий, например: [\"камень\", \"бумага\", \"ножницы\", \"камень\", \"ножницы\", \"бумага\"].\n",
    "\n",
    "Описание: Повторяет эту последовательность циклически, независимо от действий противника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fixed_sequence_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fixed_sequence_agent.py\n",
    "\n",
    "def fixed_sequence_agent(observation, configuration):\n",
    "    sequence = [0, 1, 2, 0, 2, 1]  # Предопределенная последовательность\n",
    "    return sequence[observation.step % len(sequence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия 15: Предсказывает следующее действие противника на основе его предыдущих действий с использованием матрицы переходов (Марковской цепи).\n",
    "\n",
    "Описание: Строит статистическую модель поведения противника и выбирает действие, которое побеждает ожидаемый следующий ход противника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting markov_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile markov_agent.py\n",
    "\n",
    "import random\n",
    "\n",
    "def markov_agent(observation, configuration):\n",
    "    if observation.step == 0:\n",
    "        markov_agent.prev_action = None\n",
    "        markov_agent.transition_matrix = [[1/3]*3 for _ in range(3)]\n",
    "        return random.randrange(0, configuration.signs)\n",
    "    else:\n",
    "        if markov_agent.prev_action is not None:\n",
    "            last = markov_agent.prev_action\n",
    "            curr = observation.lastOpponentAction\n",
    "            markov_agent.transition_matrix[last][curr] += 1\n",
    "            total = sum(markov_agent.transition_matrix[last])\n",
    "            probs = [count / total for count in markov_agent.transition_matrix[last]]\n",
    "            predicted = probs.index(max(probs))\n",
    "            action = (predicted + 1) % configuration.signs\n",
    "        else:\n",
    "            action = random.randrange(0, configuration.signs)\n",
    "        markov_agent.prev_action = observation.lastOpponentAction\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExgIpXUVjbjN"
   },
   "source": [
    "После того, как все агенты созданы, можно организовать турнир между ними. Для этого используется функция evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rock Agent</th>\n",
       "      <th>Paper Agent</th>\n",
       "      <th>Scissors Agent</th>\n",
       "      <th>Random Agent</th>\n",
       "      <th>Cycle Agent</th>\n",
       "      <th>Beat Last Move Agent</th>\n",
       "      <th>Copy Opponent Agent</th>\n",
       "      <th>Lose to Last Move Agent</th>\n",
       "      <th>Frequency Agent</th>\n",
       "      <th>Anti-Frequency Agent</th>\n",
       "      <th>Mirror Agent</th>\n",
       "      <th>Tit-for-Tat Agent</th>\n",
       "      <th>Biased Random Agent</th>\n",
       "      <th>Fixed Sequence Agent</th>\n",
       "      <th>Markov Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rock Agent</th>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-98</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>-99</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper Agent</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-97</td>\n",
       "      <td>-1</td>\n",
       "      <td>99</td>\n",
       "      <td>-98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>-96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scissors Agent</th>\n",
       "      <td>-99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>-97</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-27</td>\n",
       "      <td>0</td>\n",
       "      <td>-98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Agent</th>\n",
       "      <td>16</td>\n",
       "      <td>-4</td>\n",
       "      <td>-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-5</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cycle Agent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>-98</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beat Last Move Agent</th>\n",
       "      <td>98</td>\n",
       "      <td>97</td>\n",
       "      <td>99</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>-49</td>\n",
       "      <td>49</td>\n",
       "      <td>96</td>\n",
       "      <td>-1</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>-48</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Copy Opponent Agent</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-16</td>\n",
       "      <td>-99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lose to Last Move Agent</th>\n",
       "      <td>-98</td>\n",
       "      <td>-99</td>\n",
       "      <td>-97</td>\n",
       "      <td>-12</td>\n",
       "      <td>98</td>\n",
       "      <td>49</td>\n",
       "      <td>-49</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>-19</td>\n",
       "      <td>98</td>\n",
       "      <td>-49</td>\n",
       "      <td>-11</td>\n",
       "      <td>50</td>\n",
       "      <td>-96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency Agent</th>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>97</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>-49</td>\n",
       "      <td>21</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anti-Frequency Agent</th>\n",
       "      <td>-97</td>\n",
       "      <td>0</td>\n",
       "      <td>-99</td>\n",
       "      <td>-3</td>\n",
       "      <td>99</td>\n",
       "      <td>-96</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>-98</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>-6</td>\n",
       "      <td>83</td>\n",
       "      <td>-97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mirror Agent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>99</td>\n",
       "      <td>-1</td>\n",
       "      <td>97</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>-12</td>\n",
       "      <td>51</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tit-for-Tat Agent</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>-98</td>\n",
       "      <td>-49</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>-21</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>-2</td>\n",
       "      <td>-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biased Random Agent</th>\n",
       "      <td>-9</td>\n",
       "      <td>-21</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-35</td>\n",
       "      <td>28</td>\n",
       "      <td>-7</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fixed Sequence Agent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-12</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>-50</td>\n",
       "      <td>0</td>\n",
       "      <td>-83</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markov Agent</th>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>97</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-31</td>\n",
       "      <td>34</td>\n",
       "      <td>99</td>\n",
       "      <td>40</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Rock Agent  Paper Agent  Scissors Agent  \\\n",
       "Rock Agent                        0          -99              99   \n",
       "Paper Agent                      99            0             -99   \n",
       "Scissors Agent                  -99           99               0   \n",
       "Random Agent                     16           -4             -18   \n",
       "Cycle Agent                       0            0               0   \n",
       "Beat Last Move Agent             98           97              99   \n",
       "Copy Opponent Agent               0            1               1   \n",
       "Lose to Last Move Agent         -98          -99             -97   \n",
       "Frequency Agent                  99           98              97   \n",
       "Anti-Frequency Agent            -97            0             -99   \n",
       "Mirror Agent                      0            0               0   \n",
       "Tit-for-Tat Agent                 0           -1               1   \n",
       "Biased Random Agent              -9          -21              22   \n",
       "Fixed Sequence Agent              0            0               0   \n",
       "Markov Agent                     97           98              97   \n",
       "\n",
       "                         Random Agent  Cycle Agent  Beat Last Move Agent  \\\n",
       "Rock Agent                          2            0                   -98   \n",
       "Paper Agent                        -2            0                   -97   \n",
       "Scissors Agent                      2            0                   -99   \n",
       "Random Agent                        0            2                   -12   \n",
       "Cycle Agent                        -2            0                     0   \n",
       "Beat Last Move Agent               -6            0                     0   \n",
       "Copy Opponent Agent               -16          -99                    99   \n",
       "Lose to Last Move Agent           -12           98                    49   \n",
       "Frequency Agent                    -3            0                   -49   \n",
       "Anti-Frequency Agent               -3           99                   -96   \n",
       "Mirror Agent                       -2           99                    -1   \n",
       "Tit-for-Tat Agent                  -8          -98                   -49   \n",
       "Biased Random Agent                 0           -1                     5   \n",
       "Fixed Sequence Agent              -12            0                    48   \n",
       "Markov Agent                       -4           -1                   -31   \n",
       "\n",
       "                         Copy Opponent Agent  Lose to Last Move Agent  \\\n",
       "Rock Agent                                 1                       98   \n",
       "Paper Agent                               -1                       99   \n",
       "Scissors Agent                             0                       97   \n",
       "Random Agent                             -11                        0   \n",
       "Cycle Agent                               99                      -98   \n",
       "Beat Last Move Agent                      50                      -49   \n",
       "Copy Opponent Agent                        0                       50   \n",
       "Lose to Last Move Agent                  -49                        0   \n",
       "Frequency Agent                           21                       99   \n",
       "Anti-Frequency Agent                      -1                       19   \n",
       "Mirror Agent                              97                      -99   \n",
       "Tit-for-Tat Agent                          1                       49   \n",
       "Biased Random Agent                        1                       -1   \n",
       "Fixed Sequence Agent                       1                      -50   \n",
       "Markov Agent                              34                       99   \n",
       "\n",
       "                         Frequency Agent  Anti-Frequency Agent  Mirror Agent  \\\n",
       "Rock Agent                           -99                    97             0   \n",
       "Paper Agent                          -98                     0             0   \n",
       "Scissors Agent                       -97                    99             0   \n",
       "Random Agent                           0                     0            -1   \n",
       "Cycle Agent                            0                   -99             0   \n",
       "Beat Last Move Agent                  49                    96            -1   \n",
       "Copy Opponent Agent                   99                     0           -99   \n",
       "Lose to Last Move Agent              -99                   -19            98   \n",
       "Frequency Agent                        0                    98             0   \n",
       "Anti-Frequency Agent                 -98                     0            99   \n",
       "Mirror Agent                           0                     0             0   \n",
       "Tit-for-Tat Agent                    -21                   -99           -99   \n",
       "Biased Random Agent                  -35                    28            -7   \n",
       "Fixed Sequence Agent                   0                   -83            51   \n",
       "Markov Agent                          40                    99             1   \n",
       "\n",
       "                         Tit-for-Tat Agent  Biased Random Agent  \\\n",
       "Rock Agent                               0                    1   \n",
       "Paper Agent                              1                   10   \n",
       "Scissors Agent                          -1                  -27   \n",
       "Random Agent                             2                   -4   \n",
       "Cycle Agent                             98                  -10   \n",
       "Beat Last Move Agent                    49                   14   \n",
       "Copy Opponent Agent                      1                   -1   \n",
       "Lose to Last Move Agent                -49                  -11   \n",
       "Frequency Agent                         21                   28   \n",
       "Anti-Frequency Agent                    99                   -6   \n",
       "Mirror Agent                            98                  -12   \n",
       "Tit-for-Tat Agent                        0                   -5   \n",
       "Biased Random Agent                     -6                    0   \n",
       "Fixed Sequence Agent                     2                    5   \n",
       "Markov Agent                            32                   21   \n",
       "\n",
       "                         Fixed Sequence Agent  Markov Agent  \n",
       "Rock Agent                                  0           -96  \n",
       "Paper Agent                                 0           -96  \n",
       "Scissors Agent                              0           -98  \n",
       "Random Agent                               -5            -8  \n",
       "Cycle Agent                                 0            -1  \n",
       "Beat Last Move Agent                      -48            32  \n",
       "Copy Opponent Agent                        -2           -34  \n",
       "Lose to Last Move Agent                    50           -96  \n",
       "Frequency Agent                             0           -40  \n",
       "Anti-Frequency Agent                       83           -97  \n",
       "Mirror Agent                               51            -2  \n",
       "Tit-for-Tat Agent                          -2           -24  \n",
       "Biased Random Agent                       -10           -20  \n",
       "Fixed Sequence Agent                        0           -17  \n",
       "Markov Agent                               15             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Список имен агентов и соответствующих им файлов\n",
    "agent_files = [\n",
    "    (\"Rock Agent\", \"rock_agent.py\"),\n",
    "    (\"Paper Agent\", \"paper_agent.py\"),\n",
    "    (\"Scissors Agent\", \"scissors_agent.py\"),\n",
    "    (\"Random Agent\", \"random_agent.py\"),\n",
    "    (\"Cycle Agent\", \"cycle_agent.py\"),\n",
    "    (\"Beat Last Move Agent\", \"beat_last_move_agent.py\"),\n",
    "    (\"Copy Opponent Agent\", \"copy_opponent.py\"),\n",
    "    (\"Lose to Last Move Agent\", \"lose_to_last_move_agent.py\"),\n",
    "    (\"Frequency Agent\", \"frequency_agent.py\"),\n",
    "    (\"Anti-Frequency Agent\", \"anti_frequency_agent.py\"),\n",
    "    (\"Mirror Agent\", \"mirror_agent.py\"),\n",
    "    (\"Tit-for-Tat Agent\", \"tit_for_tat_agent.py\"),\n",
    "    (\"Biased Random Agent\", \"biased_random_agent.py\"),\n",
    "    (\"Fixed Sequence Agent\", \"fixed_sequence_agent.py\"),\n",
    "    (\"Markov Agent\", \"markov_agent.py\"),\n",
    "]\n",
    "\n",
    "# Инициализируем таблицу результатов\n",
    "agent_names = [name for name, _ in agent_files]\n",
    "results = pd.DataFrame(0, index=agent_names, columns=agent_names)\n",
    "\n",
    "# Запускаем турниры между всеми парами агентов\n",
    "for i, (name1, file1) in enumerate(agent_files):\n",
    "    for j, (name2, file2) in enumerate(agent_files):\n",
    "        if i != j:\n",
    "            outcomes = evaluate(\n",
    "                \"rps\",\n",
    "                [file1, file2],\n",
    "                configuration={\"episodeSteps\": 100, \"tieRewardThreshold\": 1},\n",
    "                num_episodes=10\n",
    "            )\n",
    "            # Извлекаем средний результат для первого агента\n",
    "            score = outcomes[0][0]\n",
    "            results.loc[name1, name2] = score\n",
    "\n",
    "# Выводим таблицу результатов\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ полученных результатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговые результаты турнира:\n",
      "Markov Agent               597\n",
      "Beat Last Move Agent       480\n",
      "Frequency Agent            469\n",
      "Mirror Agent               229\n",
      "Copy Opponent Agent          0\n",
      "Cycle Agent                -13\n",
      "Random Agent               -43\n",
      "Biased Random Agent        -54\n",
      "Fixed Sequence Agent       -55\n",
      "Rock Agent                 -94\n",
      "Anti-Frequency Agent       -98\n",
      "Scissors Agent            -124\n",
      "Paper Agent               -184\n",
      "Lose to Last Move Agent   -334\n",
      "Tit-for-Tat Agent         -355\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Подсчитываем общее количество очков для каждого агента\n",
    "total_scores = results.sum(axis=1)\n",
    "# Сортируем по убыванию\n",
    "total_scores = total_scores.sort_values(ascending=False)\n",
    "\n",
    "print(\"Итоговые результаты турнира:\")\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лидеры турнира:\n",
    "\n",
    "    Markov Agent - Предиктивные стратегии, основанные на статистическом анализе поведения противника, оказываются наиболее эффективными.\n",
    "\n",
    "    Beat Last Move Agent - Простые адаптивные стратегии, которые непосредственно контрят предыдущие действия противника, также эффективны.\n",
    "\n",
    "    Frequency Agent - Стратегии, использующие частотный анализ, эффективны против агентов с предсказуемым или несбалансированным поведением."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
